# -*- coding: utf-8 -*-
"""Speed Layer (Real-Time Processing with Spark ).py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Hs6NVXif6Flx0aPA4frFX98oCpy_tgGO
"""

# Social Media Sentiment Analysis - Speed Layer Implementation
## Setup and Dependencies
!pip install pyspark pymongo transformers torch nltk pymongo[srv]

"""
# Social Media Sentiment Analysis - Speed Layer Implementation

This script implements the speed layer for a lambda architecture that processes
social media data in real-time using Spark Structured Streaming. It analyzes sentiment
from Mastodon posts and YouTube comments, and stores the results in MongoDB.
"""

# Import required libraries
import os
import json
import pandas as pd
from datetime import datetime
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, udf, lit, to_date, current_timestamp, window
from pyspark.sql.types import StringType, ArrayType, StructType, StructField, IntegerType, FloatType, TimestampType
import pymongo
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
import nltk
from google.colab import drive, userdata

# Download necessary NLTK data
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('punkt_tab')

# Mount Google Drive to save processed data and models
from google.colab import drive
drive.mount('/content/drive')

# Set project directory
PROJECT_DIR = '/content/drive/MyDrive/Colab Notebooks/Sentiment Analysis'
os.makedirs(PROJECT_DIR, exist_ok=True)

## Initialize Spark Session with Streaming configurations
spark = SparkSession.builder \
    .appName("SentimentAnalysisSpeedLayer") \
    .config("spark.driver.memory", "12g") \
    .config("spark.executor.memory", "4g") \
    .config("spark.sql.streaming.schemaInference", "true") \
    .getOrCreate()

print(f"Spark version: {spark.version}")

## MongoDB Connection
# Get credentials secret keys
username = userdata.get('mongodb_username')
password = userdata.get('mongodb_pw')
cluster_url = "cluster0.8ad48r1.mongodb.net"
MONGO_CONNECTION_STRING = f"mongodb+srv://{username}:{password}@{cluster_url}/?retryWrites=true&w=majority&appName=Cluster0"

def connect_to_mongodb():
    """Connect to MongoDB and return the database client."""
    try:
        client = pymongo.MongoClient(MONGO_CONNECTION_STRING)
        print("Connected to MongoDB successfully!")
        return client
    except Exception as e:
        print(f"Failed to connect to MongoDB: {e}")
        return None

mongo_client = connect_to_mongodb()

# Create the new database for speed layer outputs
db = mongo_client["social_media_analytics_new"] if mongo_client else None

## Define Schemas - Needed for streaming sources
# Create schema for Mastodon data
mastodon_schema = StructType([
    StructField("tag", StringType(), True),
    StructField("text", StringType(), True),
    StructField("created_at", TimestampType(), True),
    StructField("post_url", StringType(), True)
])

# Create schema for YouTube data
youtube_schema = StructType([
    StructField("tag", StringType(), True),
    StructField("text", StringType(), True),
    StructField("published_at", StringType(), True),
    StructField("video_id", StringType(), True),
    StructField("video_title", StringType(), True)
])

## Text Preprocessing Function
def clean_text(text):
    """Clean text by removing unwanted characters, HTML tags, links, etc."""
    if not text:
        return None

    import re
    # Remove HTML tags
    text = re.sub(r'<.*?>', '', text)
    # Remove URLs
    text = re.sub(r'http\S+|www\S+|https\S+', '', text)
    # Remove mentions
    text = re.sub(r'@\w+', '', text)
    # Remove hashtag symbols but keep the text
    text = re.sub(r'#', '', text)
    # Remove special characters
    text = re.sub(r'[^\w\s]', '', text)
    # Remove extra whitespace
    text = re.sub(r'\s+', ' ', text).strip()

    return text if text else None

## Sentiment Analysis Model
def load_sentiment_model():
    """Load a pre-trained sentiment analysis model."""
    model_name = "cardiffnlp/twitter-roberta-base-sentiment"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSequenceClassification.from_pretrained(model_name)
    return tokenizer, model

tokenizer, model = load_sentiment_model()

def analyze_sentiment_transformers(text, max_length=512):
    """Analyze sentiment using Transformers."""
    if not text:
        return None

    # Truncate text if needed
    if len(text) > max_length * 4:  # rough character estimate
        text = text[:max_length * 4]

    inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=max_length)

    with torch.no_grad():
        outputs = model(**inputs)
        scores = torch.nn.functional.softmax(outputs.logits, dim=1)
        sentiment_id = torch.argmax(scores).item()

    # Map sentiment ID to label (specific to the model)
    id2label = {0: "negative", 1: "neutral", 2: "positive"}
    sentiment = id2label[sentiment_id]

    return sentiment

# Register UDFs for data processing
clean_text_udf = udf(clean_text, StringType())
sentiment_transformers_udf = udf(analyze_sentiment_transformers, StringType())

## Create streaming sources from MongoDB for Mastodon and YouTube data
def create_streaming_sources():
    """Create streaming sources that continuously pull data from MongoDB collections."""

    # Setup MongoDB connection options for Spark
    mongo_options = {
        "uri": MONGO_CONNECTION_STRING,
        "database": "social_media_analytics",
        "collection": "mastodon_sentiment_data",
    }

    # Stream Mastodon data
    mastodon_stream = spark \
        .readStream \
        .format("mongodb") \
        .options(**mongo_options) \
        .load()

    # Update options for YouTube data
    mongo_options["collection"] = "youtube_sentiment_data"

    # Stream YouTube data
    youtube_stream = spark \
        .readStream \
        .format("mongodb") \
        .options(**mongo_options) \
        .load()

    return mastodon_stream, youtube_stream

## Alternative approach without direct MongoDB connector - using socket or file source
def setup_mock_streaming_sources():
    """
    Create mock streaming sources for testing when direct MongoDB streaming is not available.
    This simulates streaming by reading from source collections in small batches.
    """
    # Define input source directory for checkpointing
    input_dir = "/content/drive/MyDrive/Colab Notebooks/Sentiment Analysis/input"
    os.makedirs(input_dir, exist_ok=True)

    # Create rate-limited stream for testing
    mastodon_stream = spark \
        .readStream \
        .format("rate") \
        .option("rowsPerSecond", 5) \
        .load() \
        .selectExpr("timestamp AS created_at", "value AS id")

    youtube_stream = spark \
        .readStream \
        .format("rate") \
        .option("rowsPerSecond", 5) \
        .load() \
        .selectExpr("timestamp AS published_at", "value AS id")

    # For testing, load batch data and simulate as stream
    original_db = mongo_client["social_media_analytics_new"]

    # Function to convert MongoDB documents to Spark DataFrame rows
    def process_mongo_data_to_stream(collection_name, schema):
        docs = list(original_db[collection_name].find())
        return spark.createDataFrame(docs, schema=schema)

    # Load sample data for testing
    mastodon_batch = process_mongo_data_to_stream("mastodon_sentiment_data", mastodon_schema)
    youtube_batch = process_mongo_data_to_stream("youtube_sentiment_data", youtube_schema)

    # Write batch data to temp files for streaming simulation
    mastodon_path = f"{input_dir}/mastodon"
    youtube_path = f"{input_dir}/youtube"

    mastodon_batch.write.mode("overwrite").parquet(mastodon_path)
    youtube_batch.write.mode("overwrite").parquet(youtube_path)

    # Create file source streams
    mastodon_stream = spark \
        .readStream \
        .schema(mastodon_schema) \
        .parquet(mastodon_path)

    youtube_stream = spark \
        .readStream \
        .schema(youtube_schema) \
        .parquet(youtube_path)

    return mastodon_stream, youtube_stream

## Data Processing Functions
def process_mastodon_stream(stream):
    """Process Mastodon streaming data."""
    if stream is None:
        return None

    # Convert timestamp to date format and rename created_at to timestamp
    stream = stream.withColumn("timestamp", col("created_at")) \
                   .withColumn("date", to_date(col("timestamp")))

    # Clean content
    stream = stream.withColumn("clean_content", clean_text_udf(col("text")))

    # Filter out empty content
    stream = stream.filter(col("clean_content").isNotNull() & (col("clean_content") != ""))

    # Add sentiment analysis
    stream = stream.withColumn("sentiment", sentiment_transformers_udf(col("clean_content")))

    # Add platform identifier
    stream = stream.withColumn("platform", lit(" mastodon"))

    # Select relevant columns to match YouTube schema
    stream = stream.select(
        "tag",
        "text",
        "timestamp",
        "date",
        "clean_content",
        "sentiment",
        "platform",
        "post_url"  # Mastodon-specific field
    )

    return stream

def process_youtube_stream(stream):
    """Process YouTube streaming data."""
    if stream is None:
        return None

    # Convert published_at to timestamp and date format
    stream = stream.withColumn("timestamp", col("published_at")) \
                   .withColumn("date", to_date(col("timestamp")))

    # Clean comment text
    stream = stream.withColumn("clean_content", clean_text_udf(col("text")))

    # Filter out empty content
    stream = stream.filter(col("clean_content").isNotNull() & (col("clean_content") != ""))

    # Add sentiment analysis
    stream = stream.withColumn("sentiment", sentiment_transformers_udf(col("clean_content")))

    # Add platform identifier
    stream = stream.withColumn("platform", lit("youtube"))

    # Select relevant columns to match Mastodon schema
    stream = stream.select(
        "tag",
        "text",
        "timestamp",
        "date",
        "clean_content",
        "sentiment",
        "platform",
        "video_id",  # YouTube-specific field
        "video_title"  # YouTube-specific field
    )

    return stream

## Data Analysis Functions - For Real-time Aggregations
def analyze_platform_sentiment(stream):
    """Analyze sentiment distribution by platform in real-time."""
    return stream \
        .groupBy("platform", "sentiment") \
        .count() \
        .withColumn("timestamp", current_timestamp())

def analyze_tag_sentiment(stream):
    """Analyze sentiment by tag in real-time."""
    return stream \
        .groupBy("platform", "tag", "sentiment") \
        .count() \
        .withColumn("timestamp", current_timestamp())

## MongoDB Sink Functions
def save_to_mongodb_platform_sentiment(batch_df, batch_id):
    """Save platform sentiment data to MongoDB."""
    if batch_df.isEmpty():
        print(f"Empty batch {batch_id} for platform sentiment, skipping...")
        return

    # Convert batch to pandas for easier processing
    pandas_df = batch_df.toPandas()

    # Format data as expected by the serving layer
    platform_sentiment_data = pandas_df.to_dict('records')

    # Create document with timestamp
    document = {
        "timestamp": datetime.now(),
        "data": platform_sentiment_data
    }

    try:
        # Delete existing data
        db.batch_platform_sentiment.delete_many({})
        # Insert new data
        db.batch_platform_sentiment.insert_one(document)
        print(f"Saved platform sentiment batch {batch_id} to MongoDB")
    except Exception as e:
        print(f"Error saving platform sentiment batch {batch_id} to MongoDB: {e}")

def save_to_mongodb_tag_sentiment(batch_df, batch_id):
    """Save tag sentiment data to MongoDB."""
    if batch_df.isEmpty():
        print(f"Empty batch {batch_id} for tag sentiment, skipping...")
        return

    # Convert batch to pandas for easier processing
    pandas_df = batch_df.toPandas()

    # Format data as expected by the serving layer
    tag_sentiment_data = pandas_df.to_dict('records')

    # Create document with timestamp
    document = {
        "timestamp": datetime.now(),
        "data": tag_sentiment_data
    }

    try:
        # Delete existing data
        db.batch_tag_sentiment.delete_many({})
        # Insert new data
        db.batch_tag_sentiment.insert_one(document)
        print(f"Saved tag sentiment batch {batch_id} to MongoDB")
    except Exception as e:
        print(f"Error saving tag sentiment batch {batch_id} to MongoDB: {e}")

## Main Execution Logic

def run_speed_layer():
    """Run the complete speed layer processing pipeline."""
    try:
        # Get data streams
        # For production with proper MongoDB connector:
        # mastodon_stream, youtube_stream = create_streaming_sources()

        # For testing/development:
        mastodon_stream, youtube_stream = setup_mock_streaming_sources()

        # Process streams
        processed_mastodon = process_mastodon_stream(mastodon_stream)
        processed_youtube = process_youtube_stream(youtube_stream)

        # Union the streams with allowMissingColumns=True to handle platform-specific fields
        combined_stream = processed_mastodon.unionByName(processed_youtube, allowMissingColumns=True)

        # Create analysis streams
        platform_sentiment_stream = analyze_platform_sentiment(combined_stream)
        tag_sentiment_stream = analyze_tag_sentiment(combined_stream)

        # Define output sinks
        platform_query = platform_sentiment_stream \
            .writeStream \
            .outputMode("complete") \
            .trigger(processingTime="10 seconds") \
            .foreachBatch(save_to_mongodb_platform_sentiment) \
            .option("checkpointLocation", f"{PROJECT_DIR}/checkpoints/platform_sentiment") \
            .start()

        tag_query = tag_sentiment_stream \
            .writeStream \
            .outputMode("complete") \
            .trigger(processingTime="10 seconds") \
            .foreachBatch(save_to_mongodb_tag_sentiment) \
            .option("checkpointLocation", f"{PROJECT_DIR}/checkpoints/tag_sentiment") \
            .start()

        # For production:
        platform_query.awaitTermination()
        tag_query.awaitTermination()

    except Exception as e:
        print(f"Error running speed layer: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    run_speed_layer()