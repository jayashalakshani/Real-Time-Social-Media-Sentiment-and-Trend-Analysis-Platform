{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOcRHwyhvBhWkpMK6UiHucf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jayashalakshani/Real-Time-Social-Media-Sentiment-and-Trend-Analysis-Platform/blob/main/Save_Mastodon_Trend_%2B_Post_Data_to_MongoDB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01-rN9D2edn7",
        "outputId": "c6d0716e-ad2e-47cf-8c99-453ecf79a603"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mastodon.py\n",
            "  Downloading mastodon_py-2.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pymongo[srv]\n",
            "  Downloading pymongo-4.12.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
            "\u001b[33mWARNING: pymongo 4.12.0 does not provide the extra 'srv'\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting dnspython<3.0.0,>=1.16.0 (from pymongo[srv])\n",
            "  Downloading dnspython-2.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: requests>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from mastodon.py) (2.32.3)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from mastodon.py) (2.8.2)\n",
            "Collecting python-magic (from mastodon.py)\n",
            "  Downloading python_magic-0.4.27-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: decorator>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mastodon.py) (4.4.2)\n",
            "Collecting blurhash>=1.1.4 (from mastodon.py)\n",
            "  Downloading blurhash-1.1.4-py2.py3-none-any.whl.metadata (769 bytes)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.4.2->mastodon.py) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.4.2->mastodon.py) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.4.2->mastodon.py) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.4.2->mastodon.py) (2025.1.31)\n",
            "Downloading mastodon_py-2.0.1-py3-none-any.whl (108 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m108.5/108.5 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading blurhash-1.1.4-py2.py3-none-any.whl (5.3 kB)\n",
            "Downloading dnspython-2.7.0-py3-none-any.whl (313 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m313.6/313.6 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pymongo-4.12.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993223 sha256=4ab655fc0a688f8f86d7b713b5984ca958c853de403da33ccf143f4c2cac4b6a\n",
            "  Stored in directory: /root/.cache/pip/wheels/0a/f2/b2/e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\n",
            "Successfully built langdetect\n",
            "Installing collected packages: blurhash, python-magic, langdetect, dnspython, pymongo, mastodon.py\n",
            "Successfully installed blurhash-1.1.4 dnspython-2.7.0 langdetect-1.0.9 mastodon.py-2.0.1 pymongo-4.12.0 python-magic-0.4.27\n"
          ]
        }
      ],
      "source": [
        "!pip install pymongo[srv] mastodon.py langdetect"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MongoDB Setup"
      ],
      "metadata": {
        "id": "rU5p7qXJjYN2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pymongo import MongoClient\n",
        "from google.colab import userdata\n",
        "# Replace these with your credentials\n",
        "username = userdata.get('mongodb_username')\n",
        "password = userdata.get('mongodb_pw')\n",
        "cluster_url = \"cluster0.8ad48r1.mongodb.net\""
      ],
      "metadata": {
        "id": "I_FGp9-Le7xo"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Full URI\n",
        "uri = f\"mongodb+srv://{username}:{password}@{cluster_url}/?retryWrites=true&w=majority&appName=Cluster0\""
      ],
      "metadata": {
        "id": "jS6HuEojiBSo"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect to MongoDB Atlas\n",
        "client = MongoClient(uri)"
      ],
      "metadata": {
        "id": "nna3GGq2iFER"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check available databases\n",
        "client.list_database_names()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVsKTr0riX3e",
        "outputId": "b83a6567-d23e-4db3-b2a8-d51767e3d442"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['social_media_analytics', 'admin', 'local']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create / select a database and collection\n",
        "db = client[\"social_media_analytics\"]"
      ],
      "metadata": {
        "id": "49aNVhdKiHUC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check the available collection\n",
        "db.list_collection_names()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IRPWaV9IgKk7",
        "outputId": "21e85a38-8ffd-41d9-fa09-e90de4472f48"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['mastodon_tags_data',\n",
              " 'youtube_unique_tag',\n",
              " 'mastodon_unique_tag',\n",
              " 'youtube_tags_data',\n",
              " 'youtube_sentiment_collection',\n",
              " 'mastodon_sentiment_data']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get trending tags at Mastodon"
      ],
      "metadata": {
        "id": "ONmqV2s5jlnZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Text preprocessing function\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download once\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Clean and preprocess text for sentiment analysis\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'https?://\\S+', '', text)\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "    text = re.sub(r'#', '', text)\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "    tokens = word_tokenize(text)\n",
        "    filtered = [word for word in tokens if word not in stop_words and len(word) > 2]\n",
        "\n",
        "    return ' '.join(filtered)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uBpmCQm97Lns",
        "outputId": "bb64b4be-0e12-44d5-e53c-78a904da52ef"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Mastodon API\n",
        "from mastodon import Mastodon\n",
        "mastodon = Mastodon(\n",
        "    access_token=userdata.get('mastodon_access_token'),\n",
        "    api_base_url=\"https://mastodon.social\"\n",
        ")"
      ],
      "metadata": {
        "id": "BvgEzG679ShZ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "from langdetect import detect\n",
        "from datetime import datetime\n",
        "\n",
        "def is_english(text):\n",
        "    try:\n",
        "        return detect(text) == 'en'\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "def extract_hashtags(text):\n",
        "    return re.findall(r\"#(\\w+)\", text)\n",
        "\n",
        "def fetch_mastodon_trending_tags(db, top_n_tags=10, fallback_post_limit=1000):\n",
        "    print(\"ğŸ“¡ Fetching Mastodon trending tags...\")\n",
        "\n",
        "    try:\n",
        "        tag_counter = Counter()\n",
        "        seen_tags = set()\n",
        "\n",
        "        # === Step 1: trending_tags ===\n",
        "        trends = mastodon.trending_tags()\n",
        "        print(f\"ğŸ“Š Found {len(trends)} trending tags from Mastodon\")\n",
        "\n",
        "        for trend in trends:\n",
        "            raw_tag = trend[\"name\"]\n",
        "            if not is_english(raw_tag):\n",
        "                continue\n",
        "\n",
        "            cleaned_tag = preprocess_text(raw_tag)\n",
        "            if not cleaned_tag or cleaned_tag in seen_tags:\n",
        "                continue\n",
        "\n",
        "            score = sum(int(day.get(\"uses\", 0)) for day in trend.get('history', []))\n",
        "\n",
        "            tag_counter[cleaned_tag] += score\n",
        "            seen_tags.add(cleaned_tag)\n",
        "\n",
        "            db.mastodon_tags_data.insert_one({\n",
        "                \"tag_raw\": raw_tag,\n",
        "                \"tag_clean\": cleaned_tag,\n",
        "                \"url\": trend[\"url\"],\n",
        "                \"score\": score,\n",
        "                \"fetched_at\": datetime.utcnow()\n",
        "            })\n",
        "\n",
        "        # === Step 2: Supplement with hashtags from public posts ===\n",
        "        if len(tag_counter) < top_n_tags:\n",
        "            print(\"ğŸª„ Supplementing with hashtags from public posts...\")\n",
        "            public_posts = mastodon.timeline_public(limit=fallback_post_limit)\n",
        "\n",
        "            for toot in public_posts:\n",
        "                raw_tags = extract_hashtags(toot[\"content\"])\n",
        "                for raw_tag in raw_tags:\n",
        "                    cleaned_tag = preprocess_text(raw_tag)\n",
        "                    if cleaned_tag and is_english(cleaned_tag) and cleaned_tag not in seen_tags:\n",
        "                        tag_counter[cleaned_tag] += 1\n",
        "                        seen_tags.add(cleaned_tag)\n",
        "                        if len(tag_counter) >= top_n_tags:\n",
        "                            break\n",
        "                if len(tag_counter) >= top_n_tags:\n",
        "                    break\n",
        "\n",
        "        # === Step 3: Final refill if still under top_n_tags ===\n",
        "        if len(tag_counter) < top_n_tags:\n",
        "            print(f\"ğŸ” Still under {top_n_tags} â€” attempting refill...\")\n",
        "            more_posts = mastodon.timeline_public(limit=300)\n",
        "            for toot in more_posts:\n",
        "                raw_tags = extract_hashtags(toot[\"content\"])\n",
        "                for raw_tag in raw_tags:\n",
        "                    cleaned_tag = preprocess_text(raw_tag)\n",
        "                    if cleaned_tag and cleaned_tag not in seen_tags:\n",
        "                        tag_counter[cleaned_tag] += 1\n",
        "                        seen_tags.add(cleaned_tag)\n",
        "                        if len(tag_counter) >= top_n_tags:\n",
        "                            break\n",
        "                if len(tag_counter) >= top_n_tags:\n",
        "                    break\n",
        "\n",
        "        # === Step 4: Insert top N into MongoDB ===\n",
        "        final_tags = tag_counter.most_common(top_n_tags)\n",
        "        print(f\"ğŸ· Final unique tag count: {len(final_tags)}\")\n",
        "\n",
        "        for tag, score in final_tags:\n",
        "            db.mastodon_unique_tag.insert_one({\n",
        "                \"tag\": tag,\n",
        "                \"score\": score,\n",
        "                \"fetched_at\": datetime.utcnow()\n",
        "            })\n",
        "\n",
        "        print(f\"âœ… Inserted {len(final_tags)} unique trending tags into 'mastodon_unique_tag'\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error: {str(e)}\")\n"
      ],
      "metadata": {
        "id": "CQG5VFB37cFI"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fetch_mastodon_trending_tags(db, top_n_tags=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltCydewn-Gso",
        "outputId": "e275fa30-7f6a-48b5-c32f-ae83c5d933dd"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“¡ Fetching Mastodon trending tags...\n",
            "ğŸ“Š Found 10 trending tags from Mastodon\n",
            "ğŸª„ Supplementing with hashtags from public posts...\n",
            "ğŸ” Still under 10 â€” attempting refill...\n",
            "ğŸ· Final unique tag count: 6\n",
            "âœ… Inserted 6 unique trending tags into 'mastodon_unique_tag'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get post comments at Mastodon"
      ],
      "metadata": {
        "id": "EqrVLjMykDb1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load trending first 10 tags\n",
        "tags = [doc[\"tag\"] for doc in db.mastodon_unique_tag.find().limit(20)]\n",
        "# tags = [doc[\"tag\"] for doc in trend_collection.find().sort(\"created_at\", 1).limit(10)]\n",
        "print(\"Trending Tags:\", tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DjkmZyX7kCDt",
        "outputId": "366fc0bb-2c55-4f49-e763-f28fc2683596"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trending Tags: ['complaintsongsorpoems', 'musicwomenwednesday', 'throwbackthursday', 'thursdayfivelist', '28yearslater', 'egyptiandeityoftheday', 'schutz_und_kontrolle_von_zivilgesellschaftlichem_engagement', 'page', 'papyrigraecaemagicae', 'divination']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "from datetime import datetime\n",
        "import time\n",
        "import random\n",
        "\n",
        "def safe_status_context(post_id, retries=3, delay=2):\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            return mastodon.status_context(post_id)\n",
        "        except MastodonServiceUnavailableError as e:\n",
        "            print(f\"âš ï¸ Attempt {attempt+1} failed with 503. Retrying in {delay} seconds...\")\n",
        "            time.sleep(delay + random.uniform(0, 1))\n",
        "    print(f\"âŒ Failed to fetch context for post {post_id} after {retries} retries.\")\n",
        "    return {\"descendants\": []}\n",
        "\n",
        "\n",
        "for tag in tags:\n",
        "    print(f\"ğŸ“Œ Searching posts for #{tag}\")\n",
        "    posts = mastodon.timeline_hashtag(tag, limit=10)\n",
        "\n",
        "    for post in posts:\n",
        "        post_id = post['id']\n",
        "        post_url = post['url']\n",
        "\n",
        "        # Fetch replies\n",
        "        context = safe_status_context(post_id)\n",
        "        comments = context['descendants']\n",
        "\n",
        "        for comment in comments:\n",
        "            comment_text = comment['content']\n",
        "            created_at = comment['created_at']\n",
        "\n",
        "            # Strip HTML\n",
        "            plain_text = BeautifulSoup(comment_text, \"html.parser\").get_text()\n",
        "\n",
        "            # Preprocess text (you should define this function)\n",
        "            cleaned_text = preprocess_text(plain_text)\n",
        "            if not cleaned_text.strip():\n",
        "                continue  # Skip empty after cleaning\n",
        "\n",
        "            # Save to DB\n",
        "            doc = {\n",
        "                \"tag\": f\"#{tag}\",\n",
        "                \"text\": cleaned_text,\n",
        "                \"created_at\": created_at,\n",
        "                \"post_url\": post_url\n",
        "            }\n",
        "            db.mastodon_sentiment_data.insert_one(doc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BjhcKA4noJ3",
        "outputId": "cb7f298b-ab93-4827-b76d-a6b124406cd4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“Œ Searching posts for #complaintsongsorpoems\n",
            "ğŸ“Œ Searching posts for #musicwomenwednesday\n",
            "ğŸ“Œ Searching posts for #throwbackthursday\n",
            "ğŸ“Œ Searching posts for #thursdayfivelist\n",
            "ğŸ“Œ Searching posts for #28yearslater\n",
            "ğŸ“Œ Searching posts for #egyptiandeityoftheday\n",
            "ğŸ“Œ Searching posts for #schutz_und_kontrolle_von_zivilgesellschaftlichem_engagement\n",
            "ğŸ“Œ Searching posts for #page\n",
            "ğŸ“Œ Searching posts for #papyrigraecaemagicae\n",
            "ğŸ“Œ Searching posts for #divination\n"
          ]
        }
      ]
    }
  ]
}