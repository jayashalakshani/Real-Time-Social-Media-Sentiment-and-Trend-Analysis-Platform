{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOhxsfKPwhTtKxiXZufPMBZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install pymongo[srv] mastodon.py langdetect\n","import os\n","import re\n","import logging\n","import nltk\n","import time\n","import random\n","from pymongo import MongoClient\n","from mastodon import Mastodon\n","from collections import Counter\n","from langdetect import detect\n","from datetime import datetime\n","from bs4 import BeautifulSoup\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","try:\n","    from google.colab import userdata\n","except ImportError:\n","    userdata = None\n","\n","# Configure logging\n","logging.basicConfig(\n","    level=logging.INFO,\n","    format='%(asctime)s [%(levelname)s] %(message)s',\n","    handlers=[\n","        logging.StreamHandler(),\n","        logging.FileHandler('mastodon_data_collection.log')\n","    ]\n",")\n","logger = logging.getLogger(__name__)\n","\n","# Download NLTK data\n","try:\n","    nltk.download('stopwords', quiet=True)\n","    nltk.download('punkt', quiet=True)\n","    nltk.download('punkt_tab', quiet=True)\n","except Exception as e:\n","    logger.error(f\"Failed to download NLTK data: {str(e)}\")\n","    raise\n","\n","stop_words = set(stopwords.words('english'))\n","\n","# MongoDB connection\n","try:\n","    username = userdata.get('mongodb_username') if userdata else os.getenv('MONGODB_USERNAME')\n","    password = userdata.get('mongodb_pw') if userdata else os.getenv('MONGODB_PW')\n","    if not username or not password:\n","        raise ValueError(\"MongoDB credentials not provided\")\n","    cluster_url = \"cluster0.8ad48r1.mongodb.net\"\n","    uri = f\"mongodb+srv://{username}:{password}@{cluster_url}/?retryWrites=true&w=majority&appName=Cluster0\"\n","    client = MongoClient(uri)\n","    db = client['social_media_analytics_new']\n","    tags_collection = db['mastodon_tags_data']\n","    unique_tag_collection = db['mastodon_unique_tag']\n","    sentiment_collection = db['mastodon_sentiment_data']\n","    logger.info(\"Connected to MongoDB\")\n","except Exception as e:\n","    logger.error(f\"Failed to connect to MongoDB: {str(e)}\")\n","    raise\n","\n","# Initialize Mastodon API\n","try:\n","    mastodon = Mastodon(\n","        access_token=userdata.get('mastodon_access_token') if userdata else os.getenv('MASTODON_ACCESS_TOKEN'),\n","        api_base_url=\"https://mastodon.social\"\n","    )\n","    logger.info(\"Connected to Mastodon API\")\n","except Exception as e:\n","    logger.error(f\"Failed to initialize Mastodon API: {str(e)}\")\n","    raise\n","\n","def preprocess_text(text):\n","    \"\"\"Clean and preprocess text for sentiment analysis.\"\"\"\n","    if not isinstance(text, str):\n","        return \"\"\n","    text = text.lower()\n","    text = re.sub(r'https?://\\S+', '', text)\n","    text = re.sub(r'@\\w+', '', text)\n","    text = re.sub(r'#', '', text)\n","    text = re.sub(r'[^\\w\\s]', '', text)\n","    tokens = word_tokenize(text)\n","    filtered = [word for word in tokens if word not in stop_words and len(word) > 2]\n","    return ' '.join(filtered)\n","\n","def is_english(text):\n","    \"\"\"Check if text is in English.\"\"\"\n","    try:\n","        return detect(text) == 'en'\n","    except:\n","        return False\n","\n","def extract_hashtags(text):\n","    \"\"\"Extract hashtags from text.\"\"\"\n","    return re.findall(r\"#(\\w+)\", text)\n","\n","def clear_collections():\n","    \"\"\"Delete all documents in mastodon_tags_data, mastodon_unique_tag, and mastodon_sentiment_data.\"\"\"\n","    try:\n","        tags_result = tags_collection.delete_many({})\n","        unique_tag_result = unique_tag_collection.delete_many({})\n","        sentiment_result = sentiment_collection.delete_many({})\n","        logger.info(f\"Deleted {tags_result.deleted_count} documents from mastodon_tags_data\")\n","        logger.info(f\"Deleted {unique_tag_result.deleted_count} documents from mastodon_unique_tag\")\n","        logger.info(f\"Deleted {sentiment_result.deleted_count} documents from mastodon_sentiment_data\")\n","    except Exception as e:\n","        logger.error(f\"Error clearing collections: {str(e)}\")\n","        raise\n","\n","def fetch_mastodon_trending_tags(top_n_tags=10, fallback_post_limit=1000):\n","    \"\"\"Fetch trending tags and store in MongoDB.\"\"\"\n","    logger.info(\"Fetching Mastodon trending tags...\")\n","    try:\n","        tag_counter = Counter()\n","        seen_tags = set()\n","\n","        # Step 1: Fetch trending tags\n","        trends = mastodon.trending_tags()\n","        logger.info(f\"Found {len(trends)} trending tags from Mastodon\")\n","\n","        for trend in trends:\n","            raw_tag = trend[\"name\"]\n","            if not is_english(raw_tag):\n","                continue\n","            cleaned_tag = preprocess_text(raw_tag)\n","            if not cleaned_tag or cleaned_tag in seen_tags:\n","                continue\n","            score = sum(int(day.get(\"uses\", 0)) for day in trend.get('history', []))\n","            tag_counter[cleaned_tag] += score\n","            seen_tags.add(cleaned_tag)\n","            tags_collection.insert_one({\n","                \"tag_raw\": raw_tag,\n","                \"tag_clean\": cleaned_tag,\n","                \"url\": trend[\"url\"],\n","                \"score\": score,\n","                \"fetched_at\": datetime.utcnow()\n","            })\n","\n","        # Step 2: Supplement with hashtags from public posts\n","        if len(tag_counter) < top_n_tags:\n","            logger.info(\"Supplementing with hashtags from public posts...\")\n","            public_posts = mastodon.timeline_public(limit=fallback_post_limit)\n","            for toot in public_posts:\n","                raw_tags = extract_hashtags(toot[\"content\"])\n","                for raw_tag in raw_tags:\n","                    cleaned_tag = preprocess_text(raw_tag)\n","                    if cleaned_tag and is_english(cleaned_tag) and cleaned_tag not in seen_tags:\n","                        tag_counter[cleaned_tag] += 1\n","                        seen_tags.add(cleaned_tag)\n","                        if len(tag_counter) >= top_n_tags:\n","                            break\n","                if len(tag_counter) >= top_n_tags:\n","                    break\n","\n","        # Step 3: Final refill if still under top_n_tags\n","        if len(tag_counter) < top_n_tags:\n","            logger.info(f\"Still under {top_n_tags} — attempting refill...\")\n","            more_posts = mastodon.timeline_public(limit=300)\n","            for toot in more_posts:\n","                raw_tags = extract_hashtags(toot[\"content\"])\n","                for raw_tag in raw_tags:\n","                    cleaned_tag = preprocess_text(raw_tag)\n","                    if cleaned_tag and is_english(cleaned_tag) and cleaned_tag not in seen_tags:\n","                        tag_counter[cleaned_tag] += 1\n","                        seen_tags.add(cleaned_tag)\n","                        if len(tag_counter) >= top_n_tags:\n","                            break\n","                if len(tag_counter) >= top_n_tags:\n","                    break\n","\n","        # Step 4: Insert top N into MongoDB\n","        final_tags = tag_counter.most_common(top_n_tags)\n","        logger.info(f\"Final unique tag count: {len(final_tags)}\")\n","        for tag, score in final_tags:\n","            unique_tag_collection.insert_one({\n","                \"tag\": tag,\n","                \"score\": score,\n","                \"fetched_at\": datetime.utcnow()\n","            })\n","        logger.info(f\"Inserted {len(final_tags)} unique trending tags into 'mastodon_unique_tag'\")\n","\n","        return [tag for tag, _ in final_tags]\n","    except Exception as e:\n","        logger.error(f\"Error fetching trending tags: {str(e)}\")\n","        return []\n","\n","def safe_status_context(post_id, retries=3, delay=2):\n","    \"\"\"Fetch post context with retries for 503 errors.\"\"\"\n","    for attempt in range(retries):\n","        try:\n","            return mastodon.status_context(post_id)\n","        except Exception as e:\n","            logger.warning(f\"Attempt {attempt+1} failed for post {post_id}: {str(e)}. Retrying in {delay} seconds...\")\n","            time.sleep(delay + random.uniform(0, 1))\n","    logger.error(f\"Failed to fetch context for post {post_id} after {retries} retries\")\n","    return {\"descendants\": []}\n","\n","def collect_mastodon_data():\n","    \"\"\"Collect Mastodon posts/comments and store in MongoDB.\"\"\"\n","    try:\n","        # Clear existing data in collections\n","        clear_collections()\n","\n","        # Fetch trending tags\n","        tags = fetch_mastodon_trending_tags(top_n_tags=10)\n","        if not tags:\n","            logger.warning(\"No trending tags found\")\n","            return\n","        logger.info(f\"Trending Tags: {tags}\")\n","\n","        # Fetch posts and comments for each tag\n","        for tag in tags:\n","            logger.info(f\"Searching posts for #{tag}\")\n","            posts = mastodon.timeline_hashtag(tag, limit=10)\n","            for post in posts:\n","                post_id = post['id']\n","                post_url = post['url']\n","                # Fetch replies\n","                context = safe_status_context(post_id)\n","                comments = context['descendants']\n","                for comment in comments:\n","                    comment_text = comment['content']\n","                    created_at = comment['created_at']\n","                    # Strip HTML\n","                    plain_text = BeautifulSoup(comment_text, \"html.parser\").get_text()\n","                    # Preprocess text\n","                    cleaned_text = preprocess_text(plain_text)\n","                    if not cleaned_text.strip():\n","                        logger.warning(f\"Skipping comment for post {post_url}: Empty after cleaning\")\n","                        continue\n","                    # Save to DB\n","                    doc = {\n","                        \"tag\": f\"#{tag}\",\n","                        \"text\": cleaned_text,\n","                        \"created_at\": created_at,\n","                        \"post_url\": post_url\n","                    }\n","                    sentiment_collection.insert_one(doc)\n","                    logger.info(f\"Inserted Mastodon comment for post: {post_url}\")\n","    except Exception as e:\n","        logger.error(f\"Error collecting Mastodon data: {str(e)}\")\n","\n","if __name__ == \"__main__\":\n","    logger.info(\"Starting Mastodon data collection\")\n","    collect_mastodon_data()"],"metadata":{"id":"mkz1T9zVVyth","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746254501138,"user_tz":-330,"elapsed":44321,"user":{"displayName":"Jayasha Lakshani","userId":"04882499980161656328"}},"outputId":"5f7a8d78-f0ac-4a4c-f739-d01bbb698588"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting mastodon.py\n","  Downloading mastodon_py-2.0.1-py3-none-any.whl.metadata (4.1 kB)\n","Collecting langdetect\n","  Downloading langdetect-1.0.9.tar.gz (981 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting pymongo[srv]\n","  Downloading pymongo-4.12.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n","\u001b[33mWARNING: pymongo 4.12.1 does not provide the extra 'srv'\u001b[0m\u001b[33m\n","\u001b[0mCollecting dnspython<3.0.0,>=1.16.0 (from pymongo[srv])\n","  Downloading dnspython-2.7.0-py3-none-any.whl.metadata (5.8 kB)\n","Requirement already satisfied: requests>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from mastodon.py) (2.32.3)\n","Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from mastodon.py) (2.9.0.post0)\n","Collecting python-magic (from mastodon.py)\n","  Downloading python_magic-0.4.27-py2.py3-none-any.whl.metadata (5.8 kB)\n","Requirement already satisfied: decorator>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mastodon.py) (4.4.2)\n","Collecting blurhash>=1.1.4 (from mastodon.py)\n","  Downloading blurhash-1.1.4-py2.py3-none-any.whl.metadata (769 bytes)\n","Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.4.2->mastodon.py) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.4.2->mastodon.py) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.4.2->mastodon.py) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.4.2->mastodon.py) (2025.4.26)\n","Downloading mastodon_py-2.0.1-py3-none-any.whl (108 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.5/108.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading blurhash-1.1.4-py2.py3-none-any.whl (5.3 kB)\n","Downloading dnspython-2.7.0-py3-none-any.whl (313 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.6/313.6 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pymongo-4.12.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n","Building wheels for collected packages: langdetect\n","  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993223 sha256=ece2ee4a75c4ea9c5242c2a185505002c979952dcfc8ec3d1090121641f9de30\n","  Stored in directory: /root/.cache/pip/wheels/0a/f2/b2/e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\n","Successfully built langdetect\n","Installing collected packages: blurhash, python-magic, langdetect, dnspython, pymongo, mastodon.py\n","Successfully installed blurhash-1.1.4 dnspython-2.7.0 langdetect-1.0.9 mastodon.py-2.0.1 pymongo-4.12.1 python-magic-0.4.27\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"afsDa_QqKWDh"},"execution_count":null,"outputs":[]}]}