# -*- coding: utf-8 -*-
"""youtube_data_collection.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CrfH77CwiWpKFb35at2Li7K5-26mySQW
"""

!pip install pymongo[srv] mastodon.py langdetect

import os
import re
import logging
import nltk
from pymongo import MongoClient
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError
from collections import Counter
from langdetect import detect
from datetime import datetime
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import time
try:
    from google.colab import userdata
except ImportError:
    userdata = None

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('youtube_data_collection.log')
    ]
)
logger = logging.getLogger(__name__)

# Download NLTK data
try:
    nltk.download('punkt', quiet=True)
    nltk.download('stopwords', quiet=True)
    nltk.download('punkt_tab', quiet=True)
except Exception as e:
    logger.error(f"Failed to download NLTK data: {str(e)}")
    raise

stop_words = set(stopwords.words('english'))

# MongoDB connection
try:
    username = userdata.get('mongodb_username') if userdata else os.getenv('MONGODB_USERNAME')
    password = userdata.get('mongodb_pw') if userdata else os.getenv('MONGODB_PW')
    if not username or not password:
        raise ValueError("MongoDB credentials not provided")
    cluster_url = "cluster0.8ad48r1.mongodb.net"
    uri = f"mongodb+srv://{username}:{password}@{cluster_url}/?retryWrites=true&w=majority&appName=Cluster0"
    client = MongoClient(uri)
    db = client['social_media_analytics_new']
    tags_collection = db['youtube_tags_data']
    unique_tag_collection = db['youtube_unique_tag']
    sentiment_collection = db['youtube_sentiment_data']
    logger.info("Connected to MongoDB")
except Exception as e:
    logger.error(f"Failed to connect to MongoDB: {str(e)}")
    raise

# Initialize YouTube API
try:
    api_key = userdata.get('YOUTUBE_API_KEY') if userdata else os.getenv('YOUTUBE_API_KEY')
    if not api_key:
        raise ValueError("YouTube API key not provided")
    youtube = build("youtube", "v3", developerKey=api_key)
    logger.info("Connected to YouTube API")
except Exception as e:
    logger.error(f"Failed to initialize YouTube API: {str(e)}")
    raise

def preprocess_text(text):
    """Clean and preprocess text for sentiment analysis."""
    if not isinstance(text, str):
        return ""
    text = text.lower()
    text = re.sub(r'https?://\S+', '', text)
    text = re.sub(r'@\w+', '', text)
    text = re.sub(r'#', '', text)
    text = re.sub(r'[^\w\s]', '', text)
    tokens = word_tokenize(text)
    filtered_tokens = [word for word in tokens if word not in stop_words and len(word) > 2]
    return ' '.join(filtered_tokens)

def is_english(text):
    """Check if text is in English."""
    try:
        return detect(text) == 'en'
    except:
        return False

def clear_collections():
    """Delete all documents in youtube_tags_data, youtube_unique_tag, and youtube_sentiment_collection."""
    try:
        tags_result = tags_collection.delete_many({})
        unique_tag_result = unique_tag_collection.delete_many({})
        sentiment_result = sentiment_collection.delete_many({})
        logger.info(f"Deleted {tags_result.deleted_count} documents from youtube_tags_data")
        logger.info(f"Deleted {unique_tag_result.deleted_count} documents from youtube_unique_tag")
        logger.info(f"Deleted {sentiment_result.deleted_count} documents from youtube_sentiment_collection")
    except Exception as e:
        logger.error(f"Error clearing collections: {str(e)}")
        raise

def fetch_youtube_trending_tags(region='US', max_results=25, top_n_tags=10):
    """Fetch trending video tags and store in MongoDB."""
    logger.info("Fetching YouTube trending tags...")
    try:
        trending_videos = youtube.videos().list(
            part="snippet",
            chart="mostPopular",
            regionCode=region,
            maxResults=max_results
        ).execute()

        tag_counter = Counter()

        for video in trending_videos["items"]:
            video_id = video["id"]
            snippet = video["snippet"]
            tags = snippet.get("tags", [])
            title = snippet["title"]
            published_at = snippet["publishedAt"]

            # Store video details
            tags_collection.insert_one({
                "video_id": video_id,
                "title": title,
                "tags": tags,
                "published_at": published_at
            })

            # Count English tags
            for tag in tags:
                tag = tag.lower()
                if is_english(tag):
                    tag_counter[tag] += 1

        # Store top N tags
        top_tags = tag_counter.most_common(top_n_tags)
        for tag, _ in top_tags:
            unique_tag_collection.insert_one({
                "tag": tag,
                "fetched_at": datetime.utcnow()
            })

        logger.info(f"Inserted top {top_n_tags} English trending tags into 'youtube_unique_tag'")
        return [tag for tag, _ in top_tags]
    except HttpError as e:
        logger.error(f"HTTP error fetching trending tags: {str(e)}")
        return []
    except Exception as e:
        logger.error(f"Error fetching trending tags: {str(e)}")
        return []

def fetch_comments_for_all_tagged_videos(min_comments=10, max_videos_per_tag=3):
    """Fetch comments for videos associated with each tag."""
    logger.info("Fetching comments for tagged videos...")
    try:
        tag_cursor = unique_tag_collection.find()
        api_calls = 0

        for tag_doc in tag_cursor:
            tag = tag_doc["tag"]
            logger.info(f"Searching videos for YouTube tag: #{tag}")

            # Search videos using the tag
            try:
                search_results = youtube.search().list(
                    q=tag,
                    part="snippet",
                    type="video",
                    maxResults=max_videos_per_tag
                ).execute()
                api_calls += 1
            except HttpError as e:
                logger.error(f"HTTP error searching videos for tag {tag}: {str(e)}")
                continue

            if not search_results["items"]:
                logger.warning(f"No videos found for tag: {tag}")
                continue

            videos_processed = 0
            for video in search_results["items"]:
                if videos_processed >= max_videos_per_tag:
                    break

                # Validate videoId
                try:
                    video_id = video["id"].get("videoId")
                    if not video_id:
                        logger.warning(f"Skipping item for tag {tag}: No videoId in search result")
                        continue
                    title = video["snippet"]["title"]
                except KeyError as e:
                    logger.warning(f"Skipping item for tag {tag}: Invalid search result structure: {str(e)}")
                    continue

                logger.info(f"Selected video: {title} (ID: {video_id})")

                # Fetch comments
                comments_collected = 0
                next_page_token = None

                try:
                    while comments_collected < min_comments:
                        response = youtube.commentThreads().list(
                            part="snippet",
                            videoId=video_id,
                            textFormat="plainText",
                            maxResults=100,
                            pageToken=next_page_token
                        ).execute()
                        api_calls += 1

                        items = response.get("items", [])
                        if not items:
                            break

                        for item in items:
                            if comments_collected >= min_comments:
                                break

                            comment_info = item["snippet"]["topLevelComment"]["snippet"]
                            comment_text = comment_info["textDisplay"]
                            author = comment_info["authorDisplayName"]
                            published_at = comment_info["publishedAt"]

                            cleaned_text = preprocess_text(comment_text)
                            if not cleaned_text.strip():
                                logger.warning(f"Skipping comment for video {video_id}: Empty after cleaning. Raw text: '{comment_text}'")
                                continue

                            sentiment_collection.insert_one({
                                "video_id": video_id,
                                "video_title": title,
                                "tag": tag,
                                "author": author,
                                "text": cleaned_text,
                                "raw_text": comment_text,
                                "published_at": published_at
                            })

                            comments_collected += 1

                        next_page_token = response.get("nextPageToken")
                        if not next_page_token:
                            break

                    logger.info(f"Collected {comments_collected} comments for '{title}'")
                    videos_processed += 1

                except HttpError as e:
                    if e.resp.status == 403 and 'commentsDisabled' in str(e):
                        logger.warning(f"Comments disabled for video {video_id} ({title}). Skipping...")
                        continue
                    elif e.resp.status == 403 and 'quotaExceeded' in str(e):
                        logger.warning(f"Quota exceeded: {str(e)}. Waiting 60 seconds...")
                        time.sleep(60)
                        continue
                    else:
                        logger.error(f"HTTP error fetching comments for video {video_id}: {str(e)}")
                        continue
                except Exception as e:
                    logger.error(f"Error fetching comments for video {video_id}: {str(e)}")
                    continue

            logger.info(f"API calls made for tag {tag}: {api_calls}")

    except Exception as e:
        logger.error(f"Error fetching comments: {str(e)}")

def collect_youtube_data():
    """Collect YouTube trending tags and comments, storing in MongoDB."""
    try:
        # Clear existing data in collections
        clear_collections()

        # Fetch trending tags
        tags = fetch_youtube_trending_tags(region="US", max_results=25, top_n_tags=10)
        if not tags:
            logger.warning("No trending tags found")
            return
        logger.info(f"Trending Tags: {tags}")

        # Fetch comments for tagged videos
        fetch_comments_for_all_tagged_videos(min_comments=10, max_videos_per_tag=3)
    except Exception as e:
        logger.error(f"Error collecting YouTube data: {str(e)}")

if __name__ == "__main__":
    logger.info("Starting YouTube data collection")
    collect_youtube_data()

