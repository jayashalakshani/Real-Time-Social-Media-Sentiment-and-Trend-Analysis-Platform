{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMYRc42MoMEYiaesklu/bx2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install pymongo[srv] mastodon.py langdetect"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4h0sJ4c2MC2g","executionInfo":{"status":"ok","timestamp":1745996365648,"user_tz":-330,"elapsed":27629,"user":{"displayName":"Jayasha Lakshani","userId":"04882499980161656328"}},"outputId":"4f09c635-b2dc-4755-ebe3-de1e2870ea8f"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting mastodon.py\n","  Downloading mastodon_py-2.0.1-py3-none-any.whl.metadata (4.1 kB)\n","Collecting langdetect\n","  Downloading langdetect-1.0.9.tar.gz (981 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting pymongo[srv]\n","  Downloading pymongo-4.12.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n","\u001b[33mWARNING: pymongo 4.12.1 does not provide the extra 'srv'\u001b[0m\u001b[33m\n","\u001b[0mCollecting dnspython<3.0.0,>=1.16.0 (from pymongo[srv])\n","  Downloading dnspython-2.7.0-py3-none-any.whl.metadata (5.8 kB)\n","Requirement already satisfied: requests>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from mastodon.py) (2.32.3)\n","Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from mastodon.py) (2.9.0.post0)\n","Collecting python-magic (from mastodon.py)\n","  Downloading python_magic-0.4.27-py2.py3-none-any.whl.metadata (5.8 kB)\n","Requirement already satisfied: decorator>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mastodon.py) (4.4.2)\n","Collecting blurhash>=1.1.4 (from mastodon.py)\n","  Downloading blurhash-1.1.4-py2.py3-none-any.whl.metadata (769 bytes)\n","Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.4.2->mastodon.py) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.4.2->mastodon.py) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.4.2->mastodon.py) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.4.2->mastodon.py) (2025.1.31)\n","Downloading mastodon_py-2.0.1-py3-none-any.whl (108 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.5/108.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading blurhash-1.1.4-py2.py3-none-any.whl (5.3 kB)\n","Downloading dnspython-2.7.0-py3-none-any.whl (313 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.6/313.6 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pymongo-4.12.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n","Building wheels for collected packages: langdetect\n","  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993223 sha256=db853673690ece3eeb2a04fc828018633c95c0590270cd5fcfbec851498988cd\n","  Stored in directory: /root/.cache/pip/wheels/0a/f2/b2/e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\n","Successfully built langdetect\n","Installing collected packages: blurhash, python-magic, langdetect, dnspython, pymongo, mastodon.py\n","Successfully installed blurhash-1.1.4 dnspython-2.7.0 langdetect-1.0.9 mastodon.py-2.0.1 pymongo-4.12.1 python-magic-0.4.27\n"]}]},{"cell_type":"code","source":["import os\n","import re\n","import logging\n","import nltk\n","from pymongo import MongoClient\n","from googleapiclient.discovery import build\n","from googleapiclient.errors import HttpError\n","from collections import Counter\n","from langdetect import detect\n","from datetime import datetime\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","import time\n","try:\n","    from google.colab import userdata\n","except ImportError:\n","    userdata = None\n","\n","# Configure logging\n","logging.basicConfig(\n","    level=logging.INFO,\n","    format='%(asctime)s [%(levelname)s] %(message)s',\n","    handlers=[\n","        logging.StreamHandler(),\n","        logging.FileHandler('youtube_data_collection.log')\n","    ]\n",")\n","logger = logging.getLogger(__name__)\n","\n","# Download NLTK data\n","try:\n","    nltk.download('punkt', quiet=True)\n","    nltk.download('stopwords', quiet=True)\n","    nltk.download('punkt_tab', quiet=True)\n","except Exception as e:\n","    logger.error(f\"Failed to download NLTK data: {str(e)}\")\n","    raise\n","\n","stop_words = set(stopwords.words('english'))\n","\n","# MongoDB connection\n","try:\n","    username = userdata.get('mongodb_username') if userdata else os.getenv('MONGODB_USERNAME')\n","    password = userdata.get('mongodb_pw') if userdata else os.getenv('MONGODB_PW')\n","    if not username or not password:\n","        raise ValueError(\"MongoDB credentials not provided\")\n","    cluster_url = \"cluster0.8ad48r1.mongodb.net\"\n","    uri = f\"mongodb+srv://{username}:{password}@{cluster_url}/?retryWrites=true&w=majority&appName=Cluster0\"\n","    client = MongoClient(uri)\n","    db = client['social_media_analytics_new']\n","    tags_collection = db['youtube_tags_data']\n","    unique_tag_collection = db['youtube_unique_tag']\n","    sentiment_collection = db['youtube_sentiment_data']\n","    logger.info(\"Connected to MongoDB\")\n","except Exception as e:\n","    logger.error(f\"Failed to connect to MongoDB: {str(e)}\")\n","    raise\n","\n","# Initialize YouTube API\n","try:\n","    api_key = userdata.get('YOUTUBE_API_KEY') if userdata else os.getenv('YOUTUBE_API_KEY')\n","    if not api_key:\n","        raise ValueError(\"YouTube API key not provided\")\n","    youtube = build(\"youtube\", \"v3\", developerKey=api_key)\n","    logger.info(\"Connected to YouTube API\")\n","except Exception as e:\n","    logger.error(f\"Failed to initialize YouTube API: {str(e)}\")\n","    raise\n","\n","def preprocess_text(text):\n","    \"\"\"Clean and preprocess text for sentiment analysis.\"\"\"\n","    if not isinstance(text, str):\n","        return \"\"\n","    text = text.lower()\n","    text = re.sub(r'https?://\\S+', '', text)\n","    text = re.sub(r'@\\w+', '', text)\n","    text = re.sub(r'#', '', text)\n","    text = re.sub(r'[^\\w\\s]', '', text)\n","    tokens = word_tokenize(text)\n","    filtered_tokens = [word for word in tokens if word not in stop_words and len(word) > 2]\n","    return ' '.join(filtered_tokens)\n","\n","def is_english(text):\n","    \"\"\"Check if text is in English.\"\"\"\n","    try:\n","        return detect(text) == 'en'\n","    except:\n","        return False\n","\n","def clear_collections():\n","    \"\"\"Delete all documents in youtube_tags_data, youtube_unique_tag, and youtube_sentiment_collection.\"\"\"\n","    try:\n","        tags_result = tags_collection.delete_many({})\n","        unique_tag_result = unique_tag_collection.delete_many({})\n","        sentiment_result = sentiment_collection.delete_many({})\n","        logger.info(f\"Deleted {tags_result.deleted_count} documents from youtube_tags_data\")\n","        logger.info(f\"Deleted {unique_tag_result.deleted_count} documents from youtube_unique_tag\")\n","        logger.info(f\"Deleted {sentiment_result.deleted_count} documents from youtube_sentiment_collection\")\n","    except Exception as e:\n","        logger.error(f\"Error clearing collections: {str(e)}\")\n","        raise\n","\n","def fetch_youtube_trending_tags(region='US', max_results=25, top_n_tags=10):\n","    \"\"\"Fetch trending video tags and store in MongoDB.\"\"\"\n","    logger.info(\"Fetching YouTube trending tags...\")\n","    try:\n","        trending_videos = youtube.videos().list(\n","            part=\"snippet\",\n","            chart=\"mostPopular\",\n","            regionCode=region,\n","            maxResults=max_results\n","        ).execute()\n","\n","        tag_counter = Counter()\n","\n","        for video in trending_videos[\"items\"]:\n","            video_id = video[\"id\"]\n","            snippet = video[\"snippet\"]\n","            tags = snippet.get(\"tags\", [])\n","            title = snippet[\"title\"]\n","            published_at = snippet[\"publishedAt\"]\n","\n","            # Store video details\n","            tags_collection.insert_one({\n","                \"video_id\": video_id,\n","                \"title\": title,\n","                \"tags\": tags,\n","                \"published_at\": published_at\n","            })\n","\n","            # Count English tags\n","            for tag in tags:\n","                tag = tag.lower()\n","                if is_english(tag):\n","                    tag_counter[tag] += 1\n","\n","        # Store top N tags\n","        top_tags = tag_counter.most_common(top_n_tags)\n","        for tag, _ in top_tags:\n","            unique_tag_collection.insert_one({\n","                \"tag\": tag,\n","                \"fetched_at\": datetime.utcnow()\n","            })\n","\n","        logger.info(f\"Inserted top {top_n_tags} English trending tags into 'youtube_unique_tag'\")\n","        return [tag for tag, _ in top_tags]\n","    except HttpError as e:\n","        logger.error(f\"HTTP error fetching trending tags: {str(e)}\")\n","        return []\n","    except Exception as e:\n","        logger.error(f\"Error fetching trending tags: {str(e)}\")\n","        return []\n","\n","def fetch_comments_for_all_tagged_videos(min_comments=10, max_videos_per_tag=3):\n","    \"\"\"Fetch comments for videos associated with each tag.\"\"\"\n","    logger.info(\"Fetching comments for tagged videos...\")\n","    try:\n","        tag_cursor = unique_tag_collection.find()\n","        api_calls = 0\n","\n","        for tag_doc in tag_cursor:\n","            tag = tag_doc[\"tag\"]\n","            logger.info(f\"Searching videos for YouTube tag: #{tag}\")\n","\n","            # Search videos using the tag\n","            try:\n","                search_results = youtube.search().list(\n","                    q=tag,\n","                    part=\"snippet\",\n","                    type=\"video\",\n","                    maxResults=max_videos_per_tag\n","                ).execute()\n","                api_calls += 1\n","            except HttpError as e:\n","                logger.error(f\"HTTP error searching videos for tag {tag}: {str(e)}\")\n","                continue\n","\n","            if not search_results[\"items\"]:\n","                logger.warning(f\"No videos found for tag: {tag}\")\n","                continue\n","\n","            videos_processed = 0\n","            for video in search_results[\"items\"]:\n","                if videos_processed >= max_videos_per_tag:\n","                    break\n","\n","                # Validate videoId\n","                try:\n","                    video_id = video[\"id\"].get(\"videoId\")\n","                    if not video_id:\n","                        logger.warning(f\"Skipping item for tag {tag}: No videoId in search result\")\n","                        continue\n","                    title = video[\"snippet\"][\"title\"]\n","                except KeyError as e:\n","                    logger.warning(f\"Skipping item for tag {tag}: Invalid search result structure: {str(e)}\")\n","                    continue\n","\n","                logger.info(f\"Selected video: {title} (ID: {video_id})\")\n","\n","                # Fetch comments\n","                comments_collected = 0\n","                next_page_token = None\n","\n","                try:\n","                    while comments_collected < min_comments:\n","                        response = youtube.commentThreads().list(\n","                            part=\"snippet\",\n","                            videoId=video_id,\n","                            textFormat=\"plainText\",\n","                            maxResults=100,\n","                            pageToken=next_page_token\n","                        ).execute()\n","                        api_calls += 1\n","\n","                        items = response.get(\"items\", [])\n","                        if not items:\n","                            break\n","\n","                        for item in items:\n","                            if comments_collected >= min_comments:\n","                                break\n","\n","                            comment_info = item[\"snippet\"][\"topLevelComment\"][\"snippet\"]\n","                            comment_text = comment_info[\"textDisplay\"]\n","                            author = comment_info[\"authorDisplayName\"]\n","                            published_at = comment_info[\"publishedAt\"]\n","\n","                            cleaned_text = preprocess_text(comment_text)\n","                            if not cleaned_text.strip():\n","                                logger.warning(f\"Skipping comment for video {video_id}: Empty after cleaning. Raw text: '{comment_text}'\")\n","                                continue\n","\n","                            sentiment_collection.insert_one({\n","                                \"video_id\": video_id,\n","                                \"video_title\": title,\n","                                \"tag\": tag,\n","                                \"author\": author,\n","                                \"text\": cleaned_text,\n","                                \"raw_text\": comment_text,\n","                                \"published_at\": published_at\n","                            })\n","\n","                            comments_collected += 1\n","\n","                        next_page_token = response.get(\"nextPageToken\")\n","                        if not next_page_token:\n","                            break\n","\n","                    logger.info(f\"Collected {comments_collected} comments for '{title}'\")\n","                    videos_processed += 1\n","\n","                except HttpError as e:\n","                    if e.resp.status == 403 and 'commentsDisabled' in str(e):\n","                        logger.warning(f\"Comments disabled for video {video_id} ({title}). Skipping...\")\n","                        continue\n","                    elif e.resp.status == 403 and 'quotaExceeded' in str(e):\n","                        logger.warning(f\"Quota exceeded: {str(e)}. Waiting 60 seconds...\")\n","                        time.sleep(60)\n","                        continue\n","                    else:\n","                        logger.error(f\"HTTP error fetching comments for video {video_id}: {str(e)}\")\n","                        continue\n","                except Exception as e:\n","                    logger.error(f\"Error fetching comments for video {video_id}: {str(e)}\")\n","                    continue\n","\n","            logger.info(f\"API calls made for tag {tag}: {api_calls}\")\n","\n","    except Exception as e:\n","        logger.error(f\"Error fetching comments: {str(e)}\")\n","\n","def collect_youtube_data():\n","    \"\"\"Collect YouTube trending tags and comments, storing in MongoDB.\"\"\"\n","    try:\n","        # Clear existing data in collections\n","        clear_collections()\n","\n","        # Fetch trending tags\n","        tags = fetch_youtube_trending_tags(region=\"US\", max_results=25, top_n_tags=10)\n","        if not tags:\n","            logger.warning(\"No trending tags found\")\n","            return\n","        logger.info(f\"Trending Tags: {tags}\")\n","\n","        # Fetch comments for tagged videos\n","        fetch_comments_for_all_tagged_videos(min_comments=10, max_videos_per_tag=3)\n","    except Exception as e:\n","        logger.error(f\"Error collecting YouTube data: {str(e)}\")\n","\n","if __name__ == \"__main__\":\n","    logger.info(\"Starting YouTube data collection\")\n","    collect_youtube_data()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ei4iP8TnNCtF","executionInfo":{"status":"ok","timestamp":1745996435580,"user_tz":-330,"elapsed":69929,"user":{"displayName":"Jayasha Lakshani","userId":"04882499980161656328"}},"outputId":"eb4877fe-64fe-4f09-dab2-680ea2d933b3"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:__main__:Skipping comment for video pRwwgMLl6Xg: Empty after cleaning. Raw text: '🤝🎯'\n","WARNING:googleapiclient.http:Encountered 403 Forbidden with reason \"commentsDisabled\"\n","WARNING:__main__:Comments disabled for video Mrz1PE0QgW8 (Fox Sports 1 - Fox Sports Live). Skipping...\n","WARNING:__main__:Skipping comment for video N0Jfx1TfUBM: Empty after cleaning. Raw text: '😢'\n","WARNING:__main__:Skipping comment for video Nq3x1AkwgpY: Empty after cleaning. Raw text: '❤❤'\n","WARNING:__main__:Skipping comment for video Nq3x1AkwgpY: Empty after cleaning. Raw text: '❤'\n","WARNING:__main__:Skipping comment for video wbls4pFGEdw: Empty after cleaning. Raw text: '😂😂😂😂😂😂'\n","WARNING:__main__:Skipping comment for video wbls4pFGEdw: Empty after cleaning. Raw text: '😂😂😂😂😅😅'\n","WARNING:__main__:Skipping comment for video wbls4pFGEdw: Empty after cleaning. Raw text: '😂😂😂😂😂😂😂'\n","WARNING:__main__:Skipping comment for video wbls4pFGEdw: Empty after cleaning. Raw text: '😂😂😂😂😂😂😂'\n","WARNING:__main__:Skipping comment for video wbls4pFGEdw: Empty after cleaning. Raw text: '😅😅😅❤'\n","WARNING:__main__:Skipping comment for video H58vbez_m4E: Empty after cleaning. Raw text: ''\n","WARNING:__main__:Skipping comment for video H58vbez_m4E: Empty after cleaning. Raw text: '🔥🔥🔥🔥🔥🔥🔥🔥🔥'\n","WARNING:__main__:Skipping comment for video H58vbez_m4E: Empty after cleaning. Raw text: 'https://youtu.be/zcmXLctIW24?si=h-vh42WQvl90-XwM'\n","WARNING:__main__:Skipping comment for video H58vbez_m4E: Empty after cleaning. Raw text: '❤🎉'\n","WARNING:__main__:Skipping comment for video TJJvVw6K27Y: Empty after cleaning. Raw text: '❤❤❤'\n","WARNING:__main__:Skipping comment for video gzrAOiFgw0c: Empty after cleaning. Raw text: '😎'\n","WARNING:__main__:Skipping comment for video OGduVGy_SzA: Empty after cleaning. Raw text: '❤❤❤❤❤❤🇧🇩🇧🇩🇧🇩🇧🇩🇧🇩'\n","WARNING:__main__:Skipping comment for video OGduVGy_SzA: Empty after cleaning. Raw text: 'Hi'\n","WARNING:__main__:Skipping item for tag nfl on fox: No videoId in search result\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"b7JNv1CAMkjb","executionInfo":{"status":"ok","timestamp":1745996435609,"user_tz":-330,"elapsed":22,"user":{"displayName":"Jayasha Lakshani","userId":"04882499980161656328"}}},"execution_count":2,"outputs":[]}]}